{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogVideoX Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from diffusers import CogVideoXPipeline\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def show_frame(tensor_frame):\n",
    "    \"\"\"Display a tensor frame for debugging purposes.\"\"\"\n",
    "    if tensor_frame.dim() == 4:  \n",
    "        tensor_frame = tensor_frame.squeeze(0)\n",
    "    img = tensor_frame.to(torch.float32).cpu().numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1) \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img.transpose(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def print_tensor_info(tensor, name=\"Tensor\"):\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(f\"{name} is not a tensor, but a {type(tensor)}\")\n",
    "        return\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Type: {tensor.dtype}\")\n",
    "    print(f\"  Device: {tensor.device}\")\n",
    "    print(f\"  Value range: [{tensor.min().item():.2f}, {tensor.max().item():.2f}]\")\n",
    "\n",
    "def print_module_info(module, name=\"Module\"):\n",
    "    print(f\"\\n{name} Information:\")\n",
    "    print(f\"Type: {type(module)}\")\n",
    "    print(f\"Device: {next(module.parameters()).device}\")\n",
    "    print(f\"Parameter dtype: {next(module.parameters()).dtype}\")\n",
    "    total_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"\\nKey attributes:\")\n",
    "    for attr_name in dir(module):\n",
    "        if not attr_name.startswith('_'):\n",
    "            try:\n",
    "                attr = getattr(module, attr_name)\n",
    "                if isinstance(attr, (int, float, str, bool, torch.dtype)):\n",
    "                    print(f\"  {attr_name}: {attr}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Simple memory cleanup for both CPU and MPS.\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache() \n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def save_features(features_dict, save_dir):\n",
    "    \"\"\"\n",
    "    Save extracted features to disk in a structured format:\n",
    "        save_dir/\n",
    "          ├── cogvideox_vae/\n",
    "          │   ├── low_features.pt\n",
    "          │   ├── mid_features.pt\n",
    "          │   ├── high_features.pt\n",
    "          │   └── latents.pt\n",
    "          └── metadata.json\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save VAE features\n",
    "    vae_features = features_dict['vae_features']\n",
    "    for level, tensor in vae_features.items():\n",
    "        save_path = os.path.join(save_dir, f\"{level}_features.pt\")\n",
    "        torch.save({\n",
    "            'tensor': tensor,\n",
    "            'dtype': str(tensor.dtype),\n",
    "            'device': str(tensor.device),\n",
    "            'shape': tensor.shape\n",
    "        }, save_path)\n",
    "    \n",
    "    # Save latents\n",
    "    latents_path = os.path.join(save_dir, \"latents.pt\")\n",
    "    torch.save({\n",
    "        'tensor': features_dict['latents'],\n",
    "        'dtype': str(features_dict['latents'].dtype),\n",
    "        'device': str(features_dict['latents'].device),\n",
    "        'shape': features_dict['latents'].shape\n",
    "    }, latents_path)\n",
    "    \n",
    "    # Save metadata about the extraction\n",
    "    metadata = {\n",
    "        'extraction_date': datetime.now().isoformat(),\n",
    "        'feature_shapes': {\n",
    "            'conv_in': list(vae_features['conv_in'].shape),\n",
    "            'down_block0': list(vae_features['down_block0'].shape),\n",
    "            'down_block1': list(vae_features['down_block1'].shape),\n",
    "            'down_block2': list(vae_features['down_block2'].shape),\n",
    "            'down_block3': list(vae_features['down_block3'].shape),\n",
    "            'mid_block': list(vae_features['mid_block'].shape),\n",
    "            'latents': list(features_dict['latents'].shape)\n",
    "        },\n",
    "        'dtypes': {\n",
    "            'conv_in': str(vae_features['conv_in'].dtype),\n",
    "            'down_block0': str(vae_features['down_block0'].dtype),\n",
    "            'down_block1': str(vae_features['down_block1'].dtype),\n",
    "            'down_block2': str(vae_features['down_block2'].dtype),\n",
    "            'down_block3': str(vae_features['down_block3'].dtype),\n",
    "            'mid_block': str(vae_features['mid_block'].dtype),\n",
    "            'latents': str(features_dict['latents'].dtype)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    metadata_path = os.path.join(save_dir, \"metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFeatures saved successfully in {save_dir}\")\n",
    "    print(\"\\nSaved files:\")\n",
    "    for root, _, files in os.walk(save_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  {os.path.relpath(file_path, save_dir):<30} {size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "def load_features(save_dir):\n",
    "    \"\"\"\n",
    "    Load previously saved features from disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"metadata.json\"), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    vae_features = {}\n",
    "    for level in ['conv_in', 'down_block0', 'down_block1', 'down_block2', 'down_block3', 'mid_block']:\n",
    "        path = os.path.join(save_dir, f\"{level}_features.pt\")\n",
    "        data = torch.load(path)\n",
    "        vae_features[level] = data['tensor']\n",
    "    \n",
    "    latents_data = torch.load(os.path.join(save_dir, \"latents.pt\"))\n",
    "    latents = latents_data['tensor']\n",
    "    \n",
    "    return {\n",
    "        'vae_features': vae_features,\n",
    "        'latents': latents,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "class VideoFrameLoader:\n",
    "    def __init__(self, frame_dir, dtype=torch.float16): \n",
    "        \"\"\"Initialize frame loader with CogVideoX-specific dimensions.\"\"\"\n",
    "        self.frame_dir = frame_dir\n",
    "        self.frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith('.jpg')])\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((480, 720), antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    def load_frames(self, num_frames=None, verbose=True):\n",
    "        if num_frames is None:\n",
    "            num_frames = len(self.frame_files)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loading {num_frames} frames from {self.frame_dir}\")\n",
    "        \n",
    "        frames = []\n",
    "        for i in tqdm(range(min(num_frames, len(self.frame_files))), disable=not verbose):\n",
    "            img_path = os.path.join(self.frame_dir, self.frame_files[i])\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            frames.append(self.transform(img))\n",
    "        \n",
    "        frames_tensor = torch.stack(frames)\n",
    "        frames_tensor = frames_tensor.to(self.dtype)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loaded frames tensor with shape: {frames_tensor.shape}\")\n",
    "            show_frame(frames_tensor[0]) \n",
    "        return frames_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Hooks the VAE encoder to get low/mid/high features, plus latents from `vae.encode(...)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline, device='mps'):\n",
    "        self.vae = pipeline.vae.to(device)\n",
    "        self.device = device\n",
    "        self.features = {}\n",
    "        self.hook_handles = []\n",
    "\n",
    "        self.target_layers = {\n",
    "            'conv_in': ['encoder.conv_in'],\n",
    "            'down_block0': ['encoder.down_blocks.0'],\n",
    "            'down_block1': ['encoder.down_blocks.1'],\n",
    "            'down_block2': ['encoder.down_blocks.2'],\n",
    "            'down_block3': ['encoder.down_blocks.3'],\n",
    "            'mid_block': ['encoder.mid_block']\n",
    "        }\n",
    "        self._register_hooks()\n",
    "        \n",
    "    def _hook_fn(self, layer_name):\n",
    "        def hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                feat = out[0]\n",
    "            else:\n",
    "                feat = out\n",
    "            if isinstance(feat, torch.Tensor):\n",
    "                self.features[layer_name] = feat.detach()\n",
    "            else:\n",
    "                print(f\"Warning: {layer_name} output is not a tensor but {type(feat)}\")\n",
    "        return hook\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        for level, layers in self.target_layers.items():\n",
    "            for layer_name in layers:\n",
    "                module = self.vae\n",
    "                for part in layer_name.split('.'):\n",
    "                    module = getattr(module, part)\n",
    "                handle = module.register_forward_hook(self._hook_fn(layer_name))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, frames, verbose=True):\n",
    "        \"\"\"\n",
    "        frames shape: [T, 3, 480, 720]\n",
    "        reshape to [B=1, C=3, T, H=480, W=720] for VAE encoder.\n",
    "        \"\"\"\n",
    "        frames = frames.to(self.device)\n",
    "        if verbose:\n",
    "            print_tensor_info(frames, \"Input frames\")\n",
    "        \n",
    "\n",
    "        #need  T×H×W×C\n",
    "        frames_5d = frames.unsqueeze(0).permute(0, 2, 1, 3, 4).contiguous()  \n",
    "        if verbose:\n",
    "            print_tensor_info(frames_5d, \"Reshaped frames for VAE encoder\")\n",
    "        \n",
    "        # forward pass\n",
    "        encoder_output = self.vae.encode(frames_5d)\n",
    "        if verbose:\n",
    "            print(f\"Encoder output type: {type(encoder_output)}\")\n",
    "        latent_dist = encoder_output[0]\n",
    "        if verbose:\n",
    "            print(f\"Latent distribution type: {type(latent_dist)}\")\n",
    "        latents = latent_dist.sample()\n",
    "        if verbose:\n",
    "            print_tensor_info(latents, \"Sampled latents\")\n",
    "\n",
    "        # gather the features\n",
    "        features_dict = {}\n",
    "        for level, layer_names in self.target_layers.items():\n",
    "            for ln in layer_names:\n",
    "                if ln in self.features:\n",
    "                    features_dict[level] = self.features[ln]\n",
    "                    if verbose:\n",
    "                        print(f\"\\n{level}-level features from {ln}:\")\n",
    "                        print_tensor_info(features_dict[level], f\"{level} features\")\n",
    "        \n",
    "        return features_dict, latents\n",
    "    \n",
    "def extract_and_save_vae_features(frame_dir, save_dir, num_frames=None, device='mps'):\n",
    "    print(\"\\n=== PHASE 1: Extracting VAE Encoder Features ===\")\n",
    "    pipeline  = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "    # Load frames\n",
    "    loader = VideoFrameLoader(frame_dir, dtype=torch.float16)\n",
    "    frames = loader.load_frames(num_frames=num_frames)\n",
    "\n",
    "    # Hook + Extract\n",
    "    vae_extractor = VAEFeatureExtractor(pipeline, device)\n",
    "    vae_features, latents = vae_extractor.extract_features(frames, verbose=True)\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in vae_extractor.hook_handles:\n",
    "        h.remove()\n",
    "\n",
    "    # Save\n",
    "    features_dict = {\n",
    "        'vae_features': vae_features,\n",
    "        'latents': latents\n",
    "    }\n",
    "    save_features(features_dict, save_dir)\n",
    "\n",
    "    print(f\"\\n[PHASE 1 DONE] VAE features + latents saved to: {save_dir}\")\n",
    "    clear_memory()\n",
    "\n",
    "frame_dir = \"data/29\"\n",
    "save_dir = \"features/29\"\n",
    "\n",
    "# Phase 1: Extract VAE encoder features from the real frames\n",
    "extract_and_save_vae_features(\n",
    "    frame_dir=frame_dir,\n",
    "    save_dir=save_dir,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "print(\"\\nFeature extraction phase 1 complete: VAE encoder features + latents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Noise to Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_latents(latents_path, save_dir, timestep=0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Adds a controlled amount of noise to VAE latents for diffusion processing.\n",
    "    \n",
    "    Args:\n",
    "        latents_path: Path to the saved latents tensor\n",
    "        save_dir: Directory to save the noisy latents\n",
    "        timestep: Diffusion timestep (0 = minimal noise, 999 = pure noise)\n",
    "        device: Computing device to use\n",
    "    \n",
    "    The noise schedule follows the standard diffusion process where:\n",
    "    - At t=0, we have almost no noise (mostly original signal)\n",
    "    - At t=999, we have pure noise\n",
    "    We use t=0 for feature extraction since we want to preserve most of the original signal\n",
    "    while still engaging the denoising behavior of the transformer.\n",
    "    \"\"\"\n",
    "    # Load the original latents\n",
    "    latents_data = torch.load(latents_path)\n",
    "    latents = latents_data['tensor'].to(device)\n",
    "    \n",
    "    # Print original latents info\n",
    "    print(f\"Original latents shape: {latents.shape}\")\n",
    "    print(f\"Original latents range: [{latents.min():.3f}, {latents.max():.3f}]\")\n",
    "\n",
    "    # Shortcut for diffusion noise with 1 timestep \n",
    "    alpha = torch.tensor(0.999).to(device)\n",
    "\n",
    "    # Generate random noise with same shape as latents\n",
    "    noise = torch.randn_like(latents)\n",
    "    \n",
    "    # Add noise following the diffusion equation\n",
    "    noisy_latents = alpha * latents + (1-alpha) * noise\n",
    "    \n",
    "    print(f\"Noisy latents range: [{noisy_latents.min():.3f}, {noisy_latents.max():.3f}]\")\n",
    "    \n",
    "    # Save the noisy latents\n",
    "    noisy_save_path = os.path.join(save_dir, 'noisy_latents.pt')\n",
    "    torch.save({\n",
    "        'tensor': noisy_latents,\n",
    "        'dtype': str(noisy_latents.dtype),\n",
    "        'device': str(noisy_latents.device),\n",
    "        'shape': noisy_latents.shape,\n",
    "        'timestep': timestep,\n",
    "        'alpha': alpha.item(),\n",
    "    }, noisy_save_path)\n",
    "    \n",
    "    print(f\"\\nNoisy latents saved to: {noisy_save_path}\")\n",
    "    print(f\"Added noise at timestep {timestep}\")\n",
    "    print(f\"Alpha bar (noise level): {alpha.item():.6f}\")\n",
    "    \n",
    "    return noisy_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_path = \"features/horsejump-high/cogvideox_vae/latents.pt\"\n",
    "save_dir = \"features/horsejump-high/cogvideox_vae\"\n",
    "noisy_latents = add_noise_to_latents(latents_path, save_dir, timestep=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/21/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/21\"\n",
    "save_dir = \"features/21\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A kitesurfer glides dynamically across turquoise waters with a mountainous coastline in the background. Harnessing the wind's power through control lines, they maintain a steady sideways stance while generating impressive spray patterns. As they carve through the water, their board creates a widening wake of white foam. The rider's body remains balanced and tensed, arms extended to steer the kite, while repeatedly lifting off the water's surface in controlled jumps. Their fluid movements demonstrate skilled maneuvering as they navigate the conditions, with each jump producing increasingly dramatic splashes and water patterns beneath them.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/20/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/20\"\n",
    "save_dir = \"features/20\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"Three lab-coated figures stand in stylized poses on an artificial turf field, with a chain-link fence and trees visible behind them. All wear safety goggles and white attire, differentiated by their boots - one in white sneakers, one in dark brown boots, and one in tan boots. The leftmost person checks their phone while slightly swaying, creating subtle movements. The middle and rightmost figures maintain steady stances but exhibit small postural adjustments, their long hair gently moving in the breeze. Their positioning suggests a coordinated photoshoot, with each person's micro-movements contributing to a dynamic yet controlled composition against the outdoor setting.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/19/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/19\"\n",
    "save_dir = \"features/19\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A person wearing tactical gear, including a dark jacket with red accents, slides down a thick rope while holding it with one hand and gripping what appears to be a tactical weapon in the other. They display a consistently bright, enthusiastic grin throughout their descent. As they move downward, bullet shells scatter around them, catching the warm lighting that illuminates the scene. The background features vertical metallic structures and industrial-looking elements. Their motion is dynamic and controlled, maintaining balance while smoothly descending, with their equipment and clothing showing subtle movement from the action.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/18/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/18\"\n",
    "save_dir = \"features/18\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A cyclist in athletic wear and a cap performs maintenance on a road bike indoors. Standing barefoot, they begin by examining the pedals and drivetrain, leaning in close to inspect the components. Their movements are methodical as they rotate the cranks and check the chain tension. Next, they turn their attention to the front wheel, carefully lifting and spinning it to assess alignment. The cyclist continues their inspection by examining both wheels thoroughly, demonstrating careful attention to detail throughout the maintenance routine. The scene takes place in a room with wooden flooring, brick walls, and white stairs in the background.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/17/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/17\"\n",
    "save_dir = \"features/17\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A cream-colored dog with a fluffy coat explores an outdoor area with dry terrain and scattered vegetation. The dog moves deliberately across the ground, keeping its nose low while walking steadily forward. Its tail is held upright as it appears to be tracking or investigating something of interest. The dog maintains a focused, alert posture throughout its movement, suggesting it may be following a scent trail. The surroundings include a mesh fence in the background and some green foliage along the edges of the space. A red harness or collar is visible on the dog's body as it continues its methodical investigation of the area.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/16/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/16\"\n",
    "save_dir = \"features/16\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A black swan glides gracefully through dark green water, creating gentle ripples as it moves. The swan's distinctive features are prominently displayed - its elongated neck held in an elegant S-curve, bright red beak contrasting with its deep ebony plumage, and feathers arranged in a textured pattern across its body. The background consists of leafy green foliage hanging over a concrete embankment. The swan maintains a steady, smooth swimming motion while keeping its head position relatively stable, demonstrating the characteristic poise and grace these waterfowl are known for. Reflections dance on the water's surface as the bird propels itself forward.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/15/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/15\"\n",
    "save_dir = \"features/15\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A dromedary camel walks steadily along a sandy enclosure bordered by wooden fencing and green vegetation. The camel's movements reveal a distinctive gait pattern as it strides forward, lifting and placing each leg in a deliberate rhythm. Its beige-colored body sways gently with each step, while the prominent single hump maintains balance during locomotion. The camel's long neck extends forward as it moves, and its tail occasionally twitches. The lighting casts clear shadows beneath the animal as it traverses the enclosure, highlighting the natural, fluid motion of its muscular legs and characteristic pacing style.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/14/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/14\"\n",
    "save_dir = \"features/14\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A paraglider prepares for takeoff from a grassy mountainside overlooking a sprawling town and forested slopes below. Wearing gray protective gear, a helmet, and carrying a large backpack, they progressively build momentum by running forward while handling the control lines. As they gain speed, their movements become more dynamic, transitioning from a steady jog to increasingly powerful strides. The running motion intensifies until they achieve sufficient velocity, at which point they leap forward, their feet leaving the ground as the paragliding canopy lifts them into the air, beginning their descent toward the valley below.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/13/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/13\"\n",
    "save_dir = \"features/13\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"Five goldfish swim gracefully in a blue-lit aquarium environment, with one particularly distinctive fish having more prominent eyes and rounder features than its companions. The goldfish exhibit smooth, fluid movements as they navigate around aquatic plants and decorative elements, their orange-gold scales shimmering against the deep blue background. Each fish maintains its own swimming pattern - some staying higher in the tank, others swimming in the middle space, creating natural layers of movement. Their fins and tails flow elegantly as they glide through the water, demonstrating the characteristic fluid motion of healthy, active goldfish in their aquatic habitat.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/12/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/12\"\n",
    "save_dir = \"features/12\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A brown and white spotted cow stands in a grassy field with patches of bare muddy ground. The cow wears a dark collar with what appears to be a bell attached. The animal moves deliberately forward along a path marked by wooden posts and a thin wire or rope. The lighting remains consistent throughout, highlighting the cow's distinctive coat pattern and casting shadows beneath it. The ground shows signs of wear and use, with a mix of grass on one side and exposed dirt on the path where the cow walks. In the lower corner, a partial view of what seems to be a container or equipment with orange markings is visible.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/11/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/11\"\n",
    "save_dir = \"features/11\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"Three pigs forage together on dirt ground - a large black and white adult sow flanked by two smaller piglets, one brown and one white with brown spots. The brown piglet stays close to the middle while the spotted one explores on the left. All three pigs move methodically forward, keeping their snouts low to the ground as they search for food. The adult sow leads the way with purposeful steps, her distinctive black body contrasting with white patches around her face and legs. Their movements are synchronized and deliberate as they investigate the terrain together.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/10/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/10\"\n",
    "save_dir = \"features/10\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"People walk along a sunlit street in what appears to be India, with several women wearing traditional attire including salwar kameez and sarees in vibrant colors like yellow, pink, and red. A woman in a brown and pink outfit maintains a steady pace in the center, carrying bags as she moves forward. Behind her, motorcycles are parked while pedestrians mill about. The scene has a warm, golden atmosphere created by late afternoon sunlight filtering through trees lining the street. Signs are visible on poles, and parked cars line one side of the road as people continue their daily activities.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/9/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/9\"\n",
    "save_dir = \"features/9\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A person wearing a gray t-shirt and dark pants performs a parkour vault movement over a concrete wall. They approach the wall with momentum, placing their hands on the top surface while lifting their body upward in a fluid motion. As they elevate, their legs tuck close to their body before extending outward in a controlled manner. The movement transitions into a precise landing position where they maintain balance while absorbing the impact. The urban setting includes metal railings, stepped pathways, and apartment buildings in the background, all under a clear blue sky.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/8/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/8\"\n",
    "save_dir = \"features/8\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A person wearing a yellow jacket and khaki pants stands in a grassy field under cloudy skies, accompanied by two dogs - a larger black and white dog and a smaller tricolor dog. The person trains the dogs using treats, with both canines attentively watching and following commands. The smaller dog performs tricks including standing on its hind legs and jumping up, while the larger dog observes. Throughout their training session, the person maintains a consistent stance while dispensing treats and giving commands, demonstrating a well-practiced routine with their responsive and eager canine companions against the backdrop of a serene rural landscape.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/7/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/7\"\n",
    "save_dir = \"features/7\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"Two people ride a custom-built cart with a bright blue base and a large cylindrical tank mounted on top as they approach and traverse a wooden ramp installed on a residential street. The driver wears a helmet while a passenger stands behind them. The ramp has yellow-painted panels supported by green and orange metal struts. As the cart moves forward, it maintains steady momentum to successfully jump the ramp while spectators watch from the sidewalk, including a person in red and others observing the stunt. The cart appears to achieve a smooth takeoff and landing during its trajectory over the ramp.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/6/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/6\"\n",
    "save_dir = \"features/6\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"Two martial artists in white uniforms engage in a judo throw technique on a green training mat while others observe from the edges. The executing practitioner initiates by gripping their partner's uniform, establishing a close standing position. Through a fluid combination of pulling and rotating movements, they lift and pivot their partner off balance, causing them to become airborne in a forward flip motion. The throw culminates with the partner landing on their back on the mat while the executing practitioner maintains control through the grip, following through to a final ground position to complete the throwing technique.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/5/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/5\"\n",
    "save_dir = \"features/5\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A red drift car performs a controlled slide across a race track, leaving dark tire marks on the concrete surface. The car maintains a precise sideways angle while navigating along a barrier lined with alternating red and white safety barriers. In the background, white canopy tents and parked vehicles are visible, along with a sign. The track appears to be part of a motorsport venue with spectator areas marked by green and yellow barriers. The motion is fluid and deliberate, demonstrating skilled car control as the driver executes the drift maneuver with consistent speed and angle throughout the run.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/4/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/4\"\n",
    "save_dir = \"features/4\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A drift car navigates a curved track at the Osaka Maishima Sports Island, demonstrating precise car control through controlled sliding. The red and white vehicle enters the corner while initiating a drift, generating thick white smoke from the tires as it maintains a sideways angle through the turn. The driver skillfully modulates the throttle and steering to maintain the optimal drift angle, with tire smoke billowing consistently throughout the maneuver. The car's trajectory follows the curve of the track marked by white lines, while spectators and event infrastructure, including display screens and temporary structures, are visible in the background.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/3/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/3\"\n",
    "save_dir = \"features/3\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A breakdancer performs a dynamic handstand sequence in front of an engaged crowd on an outdoor wooden platform, with ornate building facades as the backdrop. The dancer, wearing a red sweatshirt and light jeans, begins with a controlled handstand, then transitions through a series of fluid rotations and spins while balanced on their hands. The crowd, consisting of young people in casual streetwear, forms a semicircle around the performance space, watching intently as the dancer executes complex breaking movements. The sunlit scene captures the athletic precision and artistic expression of street dance culture.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/2/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/2\"\n",
    "save_dir = \"features/2\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A dirt bike rider performs a dramatic jump sequence on an off-road track surrounded by pine forest. Wearing bright protective gear with a neon helmet, the rider approaches the jump with speed, launches into the air, and executes a series of aerial maneuvers. The green motorcycle tilts upward as it gains height, revealing the underside of the bike and demonstrating the rider's control. The sequence captures multiple angles of the airborne bike, from initial takeoff through peak elevation, showcasing both technical skill and the dynamic relationship between rider and machine in motocross sport.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/1/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/1\"\n",
    "save_dir = \"features/1\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A gray compact SUV moves steadily along an urban street, passing through a curved section of road. The vehicle maintains a consistent speed as it navigates past historic buildings with tan facades and large columns. The scene is set in what appears to be a European city, with parked cars lining the street and yellow flowers blooming in raised planters near a large stone building. A blue directional arrow sign is visible in the background, and the lighting suggests it's during daytime. The SUV's black wheels and metallic paint glisten as it progresses through the gentle curve of the street.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeatureExtractor:\n",
    "    def __init__(self, pipeline, block_indices):\n",
    "        self.transformer = pipeline.transformer\n",
    "        self.block_indices = block_indices\n",
    "        self.features = {} \n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "        self._register_final_output_hook()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for idx, block in enumerate(self.transformer.transformer_blocks):\n",
    "            if idx in self.block_indices:\n",
    "                handle = block.register_forward_hook(self._make_hook_fn(idx))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def _register_final_output_hook(self):\n",
    "        def final_hook(module, inp, out):\n",
    "            if isinstance(out, tuple):\n",
    "                # If return_dict=True, output is in .sample\n",
    "                final_output = out[0] if not hasattr(out, 'sample') else out.sample\n",
    "            else:\n",
    "                final_output = out\n",
    "            self.features['final_unpatchified'] = final_output.detach()\n",
    "        \n",
    "        # Register hook on the transformer itself\n",
    "        handle = self.transformer.register_forward_hook(final_hook)\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "\n",
    "    def _make_hook_fn(self, block_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            hidden_states, enc_hidden_states = out\n",
    "            #hidden_states = hidden_states.detach().cpu()\n",
    "            #enc_hidden_states = enc_hidden_states.detach().cpu()\n",
    "            self.features[f\"block_{block_idx}_hidden\"] = hidden_states\n",
    "            self.features[f\"block_{block_idx}_enc\"] = enc_hidden_states\n",
    "        return hook_fn\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            h.remove()\n",
    "\n",
    "    def clear_features(self):\n",
    "        self.features = {}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_and_save_transformer_features_single_step(\n",
    "    save_dir,\n",
    "    prompt,\n",
    "    block_indices,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    height,\n",
    "    width,\n",
    "    num_frames,\n",
    "    device\n",
    "):\n",
    "\n",
    "    latents = torch.load(\"features/0/latents.pt\", weights_only=True)['tensor']\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)\n",
    "    latents = latents.to(device=device, dtype=torch.float16)\n",
    "\n",
    "    pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16).to(device)\n",
    "    pipeline.transformer.eval()\n",
    "\n",
    "    # Hook the transformer blocks\n",
    "    trans_hook = TransformerFeatureExtractor(pipeline, block_indices)\n",
    "\n",
    "    # Short pipeline call with 1 step\n",
    "    try:\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=num_frames,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            latents = latents,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        transformer_features = {}\n",
    "        for key, feat in trans_hook.features.items():\n",
    "            transformer_features[key] = feat.cpu()\n",
    "\n",
    "        out_path = os.path.join(save_dir, \"cogvideox_features.pt\")\n",
    "        torch.save(transformer_features, out_path)\n",
    "        print(f\"Saved transformer features to: {out_path}\")\n",
    "\n",
    "    finally:\n",
    "        trans_hook.remove_hooks()\n",
    "        trans_hook.clear_features()\n",
    "        clear_memory()\n",
    "\n",
    "    return transformer_features\n",
    "\n",
    "frame_dir = \"data/0\"\n",
    "save_dir = \"features/0\"\n",
    "\n",
    "# Phase 2: Extract Transformer features with a single-step pipeline call\n",
    "single_step_features = extract_and_save_transformer_features_single_step(\n",
    "    save_dir=save_dir,\n",
    "    prompt=\"A mountain goat with distinctive long gray and tan fur traverses rocky terrain along a steep mountainside. The goat demonstrates remarkable agility as it carefully steps across large pink-hued boulders, maintaining perfect balance despite the precarious elevation. Its thick coat sways gently with each deliberate movement, while the rugged landscape of scattered vegetation and exposed rock formations creates a dramatic backdrop. The goat's sure-footed navigation showcases its natural adaptation to mountainous environments, as it confidently moves across the rocky outcrop while surveying the surrounding slopes and valleys below.\",\n",
    "    block_indices=(0,7,14,21,29),\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=1.0,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nFeature extraction phase 2 complete: Transformer features from single diffusion step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:\n",
    "A mountain goat with distinctive long gray and tan fur traverses rocky terrain along a steep mountainside. The goat demonstrates remarkable agility as it carefully steps across large pink-hued boulders, maintaining perfect balance despite the precarious elevation. Its thick coat sways gently with each deliberate movement, while the rugged landscape of scattered vegetation and exposed rock formations creates a dramatic backdrop. The goat's sure-footed navigation showcases its natural adaptation to mountainous environments, as it confidently moves across the rocky outcrop while surveying the surrounding slopes and valleys below.\n",
    "\n",
    "\n",
    "1:\n",
    "A gray compact SUV moves steadily along an urban street, passing through a curved section of road. The vehicle maintains a consistent speed as it navigates past historic buildings with tan facades and large columns. The scene is set in what appears to be a European city, with parked cars lining the street and yellow flowers blooming in raised planters near a large stone building. A blue directional arrow sign is visible in the background, and the lighting suggests it's during daytime. The SUV's black wheels and metallic paint glisten as it progresses through the gentle curve of the street.\n",
    "\n",
    "\n",
    "2:\n",
    "A dirt bike rider performs a dramatic jump sequence on an off-road track surrounded by pine forest. Wearing bright protective gear with a neon helmet, the rider approaches the jump with speed, launches into the air, and executes a series of aerial maneuvers. The green motorcycle tilts upward as it gains height, revealing the underside of the bike and demonstrating the rider's control. The sequence captures multiple angles of the airborne bike, from initial takeoff through peak elevation, showcasing both technical skill and the dynamic relationship between rider and machine in motocross sport.\n",
    "\n",
    "\n",
    "3:\n",
    "A breakdancer performs a dynamic handstand sequence in front of an engaged crowd on an outdoor wooden platform, with ornate building facades as the backdrop. The dancer, wearing a red sweatshirt and light jeans, begins with a controlled handstand, then transitions through a series of fluid rotations and spins while balanced on their hands. The crowd, consisting of young people in casual streetwear, forms a semicircle around the performance space, watching intently as the dancer executes complex breaking movements. The sunlit scene captures the athletic precision and artistic expression of street dance culture.\n",
    "\n",
    "4:\n",
    "A drift car navigates a curved track at the Osaka Maishima Sports Island, demonstrating precise car control through controlled sliding. The red and white vehicle enters the corner while initiating a drift, generating thick white smoke from the tires as it maintains a sideways angle through the turn. The driver skillfully modulates the throttle and steering to maintain the optimal drift angle, with tire smoke billowing consistently throughout the maneuver. The car's trajectory follows the curve of the track marked by white lines, while spectators and event infrastructure, including display screens and temporary structures, are visible in the background.\n",
    "\n",
    "5:\n",
    "A red drift car performs a controlled slide across a race track, leaving dark tire marks on the concrete surface. The car maintains a precise sideways angle while navigating along a barrier lined with alternating red and white safety barriers. In the background, white canopy tents and parked vehicles are visible, along with a \"KIDS\" sign. The track appears to be part of a motorsport venue with spectator areas marked by green and yellow barriers. The motion is fluid and deliberate, demonstrating skilled car control as the driver executes the drift maneuver with consistent speed and angle throughout the run.\n",
    "\n",
    "6:\n",
    "Two martial artists in white uniforms engage in a judo throw technique on a green training mat while others observe from the edges. The executing practitioner initiates by gripping their partner's uniform, establishing a close standing position. Through a fluid combination of pulling and rotating movements, they lift and pivot their partner off balance, causing them to become airborne in a forward flip motion. The throw culminates with the partner landing on their back on the mat while the executing practitioner maintains control through the grip, following through to a final ground position to complete the throwing technique.\n",
    "\n",
    "7:\n",
    "Two people ride a custom-built cart with a bright blue base and a large cylindrical tank mounted on top as they approach and traverse a wooden ramp installed on a residential street. The driver wears a helmet while a passenger stands behind them. The ramp has yellow-painted panels supported by green and orange metal struts. As the cart moves forward, it maintains steady momentum to successfully jump the ramp while spectators watch from the sidewalk, including a person in red and others observing the stunt. The cart appears to achieve a smooth takeoff and landing during its trajectory over the ramp.\n",
    "\n",
    "8:\n",
    "A person wearing a yellow jacket and khaki pants stands in a grassy field under cloudy skies, accompanied by two dogs - a larger black and white dog and a smaller tricolor dog. The person trains the dogs using treats, with both canines attentively watching and following commands. The smaller dog performs tricks including standing on its hind legs and jumping up, while the larger dog observes. Throughout their training session, the person maintains a consistent stance while dispensing treats and giving commands, demonstrating a well-practiced routine with their responsive and eager canine companions against the backdrop of a serene rural landscape.\n",
    "\n",
    "9:\n",
    "A person wearing a gray t-shirt and dark pants performs a parkour vault movement over a concrete wall. They approach the wall with momentum, placing their hands on the top surface while lifting their body upward in a fluid motion. As they elevate, their legs tuck close to their body before extending outward in a controlled manner. The movement transitions into a precise landing position where they maintain balance while absorbing the impact. The urban setting includes metal railings, stepped pathways, and apartment buildings in the background, all under a clear blue sky.\n",
    "\n",
    "10:\n",
    "People walk along a sunlit street in what appears to be India, with several women wearing traditional attire including salwar kameez and sarees in vibrant colors like yellow, pink, and red. A woman in a brown and pink outfit maintains a steady pace in the center, carrying bags as she moves forward. Behind her, motorcycles are parked while pedestrians mill about. The scene has a warm, golden atmosphere created by late afternoon sunlight filtering through trees lining the street. Signs are visible on poles, and parked cars line one side of the road as people continue their daily activities.\n",
    "\n",
    "11:\n",
    "Three pigs forage together on dirt ground - a large black and white adult sow flanked by two smaller piglets, one brown and one white with brown spots. The brown piglet stays close to the middle while the spotted one explores on the left. All three pigs move methodically forward, keeping their snouts low to the ground as they search for food. The adult sow leads the way with purposeful steps, her distinctive black body contrasting with white patches around her face and legs. Their movements are synchronized and deliberate as they investigate the terrain together.\n",
    "\n",
    "12:\n",
    "A brown and white spotted cow stands in a grassy field with patches of bare muddy ground. The cow wears a dark collar with what appears to be a bell attached. The animal moves deliberately forward along a path marked by wooden posts and a thin wire or rope. The lighting remains consistent throughout, highlighting the cow's distinctive coat pattern and casting shadows beneath it. The ground shows signs of wear and use, with a mix of grass on one side and exposed dirt on the path where the cow walks. In the lower corner, a partial view of what seems to be a container or equipment with orange markings is visible.\n",
    "\n",
    "13:\n",
    "Five goldfish swim gracefully in a blue-lit aquarium environment, with one particularly distinctive fish having more prominent eyes and rounder features than its companions. The goldfish exhibit smooth, fluid movements as they navigate around aquatic plants and decorative elements, their orange-gold scales shimmering against the deep blue background. Each fish maintains its own swimming pattern - some staying higher in the tank, others swimming in the middle space, creating natural layers of movement. Their fins and tails flow elegantly as they glide through the water, demonstrating the characteristic fluid motion of healthy, active goldfish in their aquatic habitat.\n",
    "\n",
    "14:\n",
    "A paraglider prepares for takeoff from a grassy mountainside overlooking a sprawling town and forested slopes below. Wearing gray protective gear, a helmet, and carrying a large backpack, they progressively build momentum by running forward while handling the control lines. As they gain speed, their movements become more dynamic, transitioning from a steady jog to increasingly powerful strides. The running motion intensifies until they achieve sufficient velocity, at which point they leap forward, their feet leaving the ground as the paragliding canopy lifts them into the air, beginning their descent toward the valley below.\n",
    "\n",
    "15:\n",
    "A dromedary camel walks steadily along a sandy enclosure bordered by wooden fencing and green vegetation. The camel's movements reveal a distinctive gait pattern as it strides forward, lifting and placing each leg in a deliberate rhythm. Its beige-colored body sways gently with each step, while the prominent single hump maintains balance during locomotion. The camel's long neck extends forward as it moves, and its tail occasionally twitches. The lighting casts clear shadows beneath the animal as it traverses the enclosure, highlighting the natural, fluid motion of its muscular legs and characteristic pacing style.\n",
    "\n",
    "16:\n",
    "A black swan glides gracefully through dark green water, creating gentle ripples as it moves. The swan's distinctive features are prominently displayed - its elongated neck held in an elegant S-curve, bright red beak contrasting with its deep ebony plumage, and feathers arranged in a textured pattern across its body. The background consists of leafy green foliage hanging over a concrete embankment. The swan maintains a steady, smooth swimming motion while keeping its head position relatively stable, demonstrating the characteristic poise and grace these waterfowl are known for. Reflections dance on the water's surface as the bird propels itself forward.\n",
    "\n",
    "17:\n",
    "A cream-colored dog with a fluffy coat explores an outdoor area with dry terrain and scattered vegetation. The dog moves deliberately across the ground, keeping its nose low while walking steadily forward. Its tail is held upright as it appears to be tracking or investigating something of interest. The dog maintains a focused, alert posture throughout its movement, suggesting it may be following a scent trail. The surroundings include a mesh fence in the background and some green foliage along the edges of the space. A red harness or collar is visible on the dog's body as it continues its methodical investigation of the area.\n",
    "\n",
    "18:\n",
    "A cyclist in athletic wear and a cap performs maintenance on a road bike indoors. Standing barefoot, they begin by examining the pedals and drivetrain, leaning in close to inspect the components. Their movements are methodical as they rotate the cranks and check the chain tension. Next, they turn their attention to the front wheel, carefully lifting and spinning it to assess alignment. The cyclist continues their inspection by examining both wheels thoroughly, demonstrating careful attention to detail throughout the maintenance routine. The scene takes place in a room with wooden flooring, brick walls, and white stairs in the background.\n",
    "\n",
    "19:\n",
    "A person wearing tactical gear, including a dark jacket with red accents, slides down a thick rope while holding it with one hand and gripping what appears to be a tactical weapon in the other. They display a consistently bright, enthusiastic grin throughout their descent. As they move downward, bullet shells scatter around them, catching the warm lighting that illuminates the scene. The background features vertical metallic structures and industrial-looking elements. Their motion is dynamic and controlled, maintaining balance while smoothly descending, with their equipment and clothing showing subtle movement from the action.\n",
    "\n",
    "20:\n",
    "Three lab-coated figures stand in stylized poses on an artificial turf field, with a chain-link fence and trees visible behind them. All wear safety goggles and white attire, differentiated by their boots - one in white sneakers, one in dark brown boots, and one in tan boots. The leftmost person checks their phone while slightly swaying, creating subtle movements. The middle and rightmost figures maintain steady stances but exhibit small postural adjustments, their long hair gently moving in the breeze. Their positioning suggests a coordinated photoshoot, with each person's micro-movements contributing to a dynamic yet controlled composition against the outdoor setting.\n",
    "\n",
    "21:\n",
    "A kitesurfer glides dynamically across turquoise waters with a mountainous coastline in the background. Harnessing the wind's power through control lines, they maintain a steady sideways stance while generating impressive spray patterns. As they carve through the water, their board creates a widening wake of white foam. The rider's body remains balanced and tensed, arms extended to steer the kite, while repeatedly lifting off the water's surface in controlled jumps. Their fluid movements demonstrate skilled maneuvering as they navigate the conditions, with each jump producing increasingly dramatic splashes and water patterns beneath them.\n",
    "\n",
    "22:\n",
    "A young cyclist rides a red bicycle along a straight path parallel to a colorful graffiti wall. Wearing a white t-shirt and patterned shorts, they maintain a steady pedaling rhythm as they cruise past the vibrant street art featuring bold orange, yellow, and green designs with stylized characters. The scene is framed by trees, with tall grass growing on the slope beside the wall. The motion blur in the images creates a sense of forward movement, while the background artwork provides a striking urban backdrop for the casual bike ride.\n",
    "\n",
    "23:\n",
    "A dancer performs a contemporary solo on an outdoor stage with a curved wall of ornamental grasses and hay bales as the backdrop. She wears a navy blue dress with flowing gray fabric attachments and dances barefoot before a seated audience. Her movements flow from standing poses into dynamic spins, with her arms gracefully extending outward and upward. She shifts her weight between feet while executing controlled turns, letting the dress fabric create sweeping patterns through the air. Her choreography includes subtle head tilts, fluid arm circles, and balanced poses that transition smoothly into swift directional changes, all while maintaining an elegant performance presence.\n",
    "\n",
    "24:\n",
    "A silver compact car navigates through an urban intersection bordered by light-colored buildings and white bollards. Starting from the right side, the vehicle smoothly curves leftward while maintaining a steady pace. As it progresses through its turn, the car's body tilts slightly due to the cornering forces, while its wheels rotate steadily against the dark asphalt. The vehicle continues its fluid arc across the frame, gradually straightening its trajectory as it approaches a pedestrian crossing. Throughout its motion, the car maintains consistent speed and exhibits stable handling characteristics while executing the left turn maneuver, eventually beginning to level out as it prepares to continue straight ahead.\n",
    "\n",
    "25:\n",
    "A small dog with distinctive black and tan coloring moves purposefully through a garden setting, passing between green metal fence posts. The dog maintains a steady trotting gait, demonstrating agile movement as it navigates around trees and past purple flowering plants. Its ears are perked upward in an alert position, and its tail is carried high while moving. The dog's muscular legs show a coordinated walking pattern, with its body remaining level and balanced throughout the motion. The background features lush greenery, including mature trees and ornamental shrubs in a fenced yard space.\n",
    "\n",
    "26:\n",
    "A motorcyclist in a navy suit and white helmet rides past a line of parked motorcycles on a sunny urban street with classical architecture featuring arches and balconies. The rider maintains a steady forward motion while seated upright, smoothly progressing from right to left across the view. Their movement appears graceful and controlled as they navigate the wide road, passing both stationary motorcycles on one side and a silver station wagon on the other. Green trees frame the top of the scene, casting dappled shadows on the street as the rider continues their journey through the frame with unwavering momentum.\n",
    "\n",
    "27:\n",
    "A motorcyclist wearing a white helmet and shirt performs a continuous burnout maneuver on a paved area near a tunnel entrance. The rider maintains steady control as the rear tire spins rapidly against the ground, generating thick white smoke that billows outward. Dark tire marks spiral and interweave across the pavement, creating intricate circular patterns as the motorcycle pivots and rotates. The smoke intensifies and spreads wider while the rider skillfully balances the motorcycle, keeping the front wheel relatively stable as the rear continues its spinning motion. The surrounding environment includes a grassy slope and concrete barriers, remaining static throughout the action.\n",
    "\n",
    "28:\n",
    "In a retail store electronics section, a store employee wearing a gray vest and blue shirt interacts with a customer in a black jacket. The employee gestures and shifts his weight while communicating, his body language suggesting an explanatory conversation. The customer, positioned to the right, carefully examines and handles display items, lifting and inspecting boxes. Their interaction flows naturally as the customer reaches for products while the employee maintains an open, engaged stance, occasionally adjusting his position and using hand movements to emphasize points. The customer's movements are deliberate and focused, suggesting careful consideration of the merchandise while the employee maintains a helpful, attentive presence throughout their exchange.\n",
    "\n",
    "29:\n",
    "A rider in white attire and black helmet guides a chestnut horse through a show jumping course. The horse's approach begins with a controlled canter, gathering momentum as they near the decorated jump obstacle. As they reach the takeoff point, the horse powerfully propels upward, tucking its front legs while the rider maintains a forward position. At the jump's apex, both horse and rider demonstrate precise form before the horse extends its legs to clear the pole. During landing, the horse's neck stretches forward while its tail flows gracefully, absorbing the impact as they transition smoothly back into a canter to continue their course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogvideox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
